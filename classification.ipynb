{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : downloading - preprocessing - uploading\n",
    "First enter the [competition](https://www.kaggle.com/c/forest-cover-type-prediction) and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/train.csv', index_col=0)\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 55 columns in the train.csv I would suggest you to create datasource from [aws website](https://aws.amazon.com/) itself, as it's interface would become easy to interact with rather than writing codes.\n",
    "\n",
    "To solve the whole problem right there on the website, you can refer [this](https://github.com/shravankumar9892/bank-marketing/blob/master/README.md).\n",
    "\n",
    "1. First save the dataset in a S3 Bucket\n",
    "2. Create the datasource\n",
    "\n",
    "![Creating Data Source](images/datasourcecreate.png)\n",
    "\n",
    "If it asks -> Does the first line in your CSV contain the column names? click on **yes**\n",
    "\n",
    "**Now the most important part comes, because of which we preferred the aws website interface**\n",
    "1. Convert all Soil_Typex features into BINARY\n",
    "2. Convert all Wilderness_Areax into BINARY\n",
    "3. Convert Cover_Type into CATEGORICAL\n",
    "\n",
    "If it asks -> Do you plan to use this dataset to create or evaluate an ML model? Click **yes**\n",
    "And then Select Cover_Type as value you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('machinelearning')\n",
    "\n",
    "__ = client.create_ml_model(\n",
    "    MLModelId='kaggleisthebest',\n",
    "    MLModelName='covertypemulticlass',\n",
    "    MLModelType='MULTICLASS',   # Amazon ML has 3 types of model types: BINARY | MULTICLASS | REGRESSION\n",
    "    TrainingDataSourceId='ds-ooCQMKjVxXp' # Replace it with your corresponding DataSource ID.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create batch predictions on the test data we again need to create a datasource\n",
    "# Remember the test.csv file has size of 72.6 mb, so it make time to create the datasource \n",
    "# But this time while creating the dataset, if they ask --> Do you plan to use this dataset to create or evaluate an ML model?\n",
    "# You should click No\n",
    "\n",
    "___ = client.create_batch_prediction(\n",
    "    BatchPredictionId='batchpredictionforest,\n",
    "    BatchPredictionName='predictresultsofforest',\n",
    "    MLModelId='kaggleisthebest',\n",
    "    BatchPredictionDataSourceId='ds-siouUHDY66y',\n",
    "    OutputUri='s3://forestcoverkaggle/'\n",
    ")\n",
    "\n",
    "# OutputUri specifies in which S3 bucket directory shall the prediction folder be placed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictions folder is generated in your S3 Bucket, download it as a csv.\n",
    "\n",
    "Now to submit it to the Kaggle competition, we need to modify the predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('data/predictions.csv')\n",
    "output = pd.read_csv('data/sampleSubmission.csv')\n",
    "\n",
    "output['Cover_Type'] = predictions.idxmax(axis = 1)\n",
    "output.to_csv('prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file delibrately had the predictive score, to make it more flexible. We did nothing but, taking maximum of each row from the predictions dataframe and then putting it in the output dataframe "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
